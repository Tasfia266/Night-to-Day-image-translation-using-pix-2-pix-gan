# -*- coding: utf-8 -*-
"""pix_2_pix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uBmC4lvTtF7cUR7GwvVwtAXZoXRs4ftC
"""

from os import listdir 
from numpy import asarray, load 
from numpy import vstack 
from keras.preprocessing.image import img_to_array 
from keras.preprocessing.image import load_img 
from numpy import savez_compressed 
from matplotlib import pyplot 
import numpy as np

from google.colab import drive 
drive.mount ('/content/drive')

def load_images (path, size= (256,512)): 
  src_list, tar_list= list(), list () 
  #enumerate filenames in directory, assume all are images 
  for filename in listdir (path): 
    # Load and resize the image 
    pixels= load_img (path+filename, target_size= size)
    #convert to numpy array 
    pixels= img_to_array (pixels) 
    #split into satellite and map 
    sat_img, map_img= pixels[:, :256], pixels[:, 256:] 
    src_list.append (sat_img) 
    tar_list.append (map_img) 
  return [asarray (src_list), asarray (tar_list)]

path= '/content/drive/MyDrive/maps/train/'

listdir(path)

[src_images, tar_images]= load_images(path)

import matplotlib.pyplot as plt
n_samples=3 
for i in range (n_samples): 
  plt.subplot (2, n_samples, i+1) 
  plt.axis ('off') 
  plt.imshow (src_images[i].astype ('uint8'))

for i in range (n_samples): 
  plt.subplot (2, n_samples, 1+n_samples+i) 
  plt.axis ('off') 
  plt.imshow (tar_images[i].astype ('uint8'))

plt.show ()



# pix to pix model

from numpy import zeros 
from numpy import ones 
from numpy.random import randint 
from tensorflow.keras.optimizers import Adam 
from keras.initializers import RandomNormal 
from keras.models import Model, Input 
from keras.layers import Conv2D 
from keras.layers import Conv2DTranspose 
from keras.layers import LeakyReLU
from keras.layers import Dropout 
from keras.layers import Activation 
from keras.layers import Concatenate 
from keras.layers import BatchNormalization 
from tensorflow.keras.utils import plot_model

def discriminator (image_shape): # c64-c128-c256-c512.after the last layer conv to 1 dimensional output followed by a sigmoid function 
  # initialize weights as described in paper 
  init= RandomNormal (stddev=0.02) 
  # Source image input 
  in_source_image= Input(shape=image_shape) 
  # Target image input 
  in_target_image= Input(shape= image_shape) 
  # concatenate images; channel wise 
  merged= Concatenate () ([in_source_image, in_target_image]) 

  d= Conv2D (64, (4,4), strides= (2,2), padding= 'same', kernel_initializer=init) (merged) 
  d= LeakyReLU (alpha=0.2)(d) 
  d= Conv2D (128, (4,4), strides= (2,2), padding= 'same', kernel_initializer=init) (d) 
  d= BatchNormalization()(d) 
  d= LeakyReLU (alpha=0.2)(d) 
  d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
  d = BatchNormalization()(d)
  d = LeakyReLU(alpha=0.2)(d)
	# C512: 4x4 kernel Stride 2x2 
  # Not in the original paper. Comment this block if you want.
  d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
  d= BatchNormalization() (d)
  d= LeakyReLU (alpha=0.2) (d)
  #second last output_layer: kernel 512 but strides (1,1) 
  d= Conv2D (512, (4,4), padding= 'same', kernel_initializer=init) (d) 
  d= BatchNormalization ()(d) 
  d= LeakyReLU(alpha=0.2)(d) 
  d= Conv2D(1, (4,4), padding= 'same', kernel_initializer= init) (d) 
  patch_out= Activation ('sigmoid') (d) 

  model=Model ([in_source_image, in_target_image], patch_out) 

  opt= Adam (lr=0.0002, beta_1= 0.5) 
  model.compile (loss= 'binary_crossentropy', optimizer=opt, loss_weights= [0.5])
  return model

def define_encoder_block (layer_in, n_filters, batch_norm=True): 
  #weight initialization 
  init= RandomNormal (stddev=0.02) 
  # add downsampling layer 
  g= Conv2D (n_filters, (4,4), strides= (2,2), padding= 'same', kernel_initializer=init) (layer_in) 
  # Conditionally add batch normalization 
  if batch_norm: 
    g= BatchNormalization()(g, training= True) 
  g= LeakyReLU(alpha=0.2)(g) 
  return g

def decoder_block (layer_in, skip_in, n_filters, dropout=True): 
  init= RandomNormal (stddev=0.02) 
  #add upsampling layer 
  g= Conv2DTranspose (n_filters, (4,4), strides= (2,2), padding= 'same', kernel_initializer=init) (layer_in)
  # add batch normalization 
  g= BatchNormalization()(g, training= True) 
  #conditionally add dropout 
  if dropout: 
    g= Dropout(0.5)(g, training=True) 
  g= Concatenate ()([g, skip_in] )
  g= Activation ('relu')(g)
  return g

def define_generator (image_shape= (256,256,3)): 
  init=RandomNormal (stddev=0.02) 
  #image input 
  in_image= Input(shape= image_shape) 
  #encoder_model. c64-c128-c256-c512-c512-c512-c512-c512 
  e1= define_encoder_block (in_image, 64, batch_norm=False) 
  e2= define_encoder_block (e1,128) 
  e3= define_encoder_block (e2, 256) 
  e4= define_encoder_block (e3, 512)
  e5= define_encoder_block (e4,512) 
  e6= define_encoder_block(e5,512) 
  e7= define_encoder_block (e6,512) 
  # no batch norm and relu
  b=Conv2D (512, (4,4), strides= (2,2), padding= 'same', kernel_initializer=init)(e7)
  b= Activation ('relu')(b) 

  #decoder model 
  d1= decoder_block (b, e7, 512)
  d2= decoder_block (d1, e6, 512) 
  d3= decoder_block (d2, e5, 512) 
  d4= decoder_block (d3, e4, 512, dropout=False) 
  d5= decoder_block (d4, e3,256, dropout=False) 
  d6= decoder_block (d5, e2, 128, dropout=False) 
  d7= decoder_block (d6, e1, 64, dropout= False)

  g= Conv2DTranspose (image_shape[2], (4,4), strides= (2,2), padding= 'same', kernel_initializer=init) (d7) 
  out= Activation ('tanh')(g) #Generates images between the range of -1, 1
  model= Model (in_image, out) 
  return model

gen_model= define_generator ((256, 256, 3))

gen_model.summary()

plot_model (gen_model)

def define_gan (g_model, d_model, image_shape):
  #make weights in discriminator not trainable 
  for layer in d_model.layers:
    if not isinstance (layer, BatchNormalization):
      layer.trainable=False
  #define the source image 
  in_source= Input (shape=image_shape) 
  # supply the image as input to the generator 
  gen_out= g_model (in_source) 
  # supply the input image and generated image as input to the discriminator 
  dis_out= d_model([in_source, gen_out]) 
  # source image as input, generated image and discriminator as output 
  model= Model (in_source, [dis_out, gen_out])
  # compile gan 
  opt= Adam (lr=0.0001, beta_1=0.5) 
  # Total loss is the weighted sum of adversarial loss and L1 loss 
  model.compile (loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100]) 
  return model

def generate_real_samples (dataset, n_samples, patch_shape): 
  train_a, train_b= dataset 
  ix= randint (0, train_a.shape[0], n_samples) 
  # retreive selected images 
  X1, X2= train_a[ix], train_b[ix] 
  #generate real class labels 
  y= ones ((n_samples, patch_shape, patch_shape, 1))
  return [X1,X2],y

def generate_fake_samples (g_model, samples, patch_shape): 
  x= g_model.predict (samples) 
  y= zeros ((len(x), patch_shape, patch_shape,1)) 
  return x,y

# Generate samples and save as a plot and save the model 
# Gan model do not converge. We just want to find a good balance between discriminator and generator 
# Therefore it makes sense to periodically save the generator model and see how the generated image looks

def summarize_performance (step, g_model, dataset, n_samples=3): 
  # Select a sample of input images 
  [x_real_a, x_real_b]= generate_real_samples (dataset, n_samples,1) 
  x_fake_b,_=generate_fake_samples (g_model, x_real_a,1) 
  #Scale all pixels from [-1,1] to [0,1] 
  x_real_a= (x_real_a+1)/2.0 
  x_real_b= (x_real_b+1)/2.0 
  x_fake_b= (x_fake_b+1)/2.0 
  #Plot real source images 
  for i in range (n_samples): 
    plt.subplot (3,n_samples,1+i) 
    plt.axis ('off')
    plt.imshow (x_real_a[i]) 
    # plot generated target image 
  for i in range (n_samples): 
    plt.subplot (3, n_samples, 1+n_samples+i)
    plt.axis ('off') 
    plt.imshow (x_fake_b[i]) 
    # plot real target image 
  for i in range (n_samples): 
    plt.subplot (3, n_samples, 1+n_samples*2+i) 
    plt.axis ('off') 
    plt.imshow (x_real_b[i])

def train (d_model, gen_model, gan_model, dataset, n_epochs=100, n_batch=1): 
  # Determine output square shape of the discriminator 
  n_patch= d_model.output_shape[1] 
  print (n_patch) 
  #Unpack dataset 
  train_a, train_b= dataset 
  # Calculate the number of batches per training epoch 
  batch_per_epoch= int (len(train_a)/n_batch) 
  # calculate the number of training iterations 
  n_steps= batch_per_epoch*n_epochs 
  # manually enumerate epochs 
  for i in range (n_steps): 
    # Select a batch of real samples 
    [x_real_a, x_real_b], y_real= generate_real_samples (dataset, n_batch, n_patch) 
    x_fake_b, y_fake= generate_fake_samples (gen_model, x_real_a, n_patch)
    # update discriminator for real samples 
    d_loss_1= d_model.train_on_batch ([x_real_a, x_real_b], y_real) 
    #Update discriminator for generated samples 
    d_loss_2= d_model.train_on_batch ([x_real_a, x_fake_b], y_fake) 
    # Update the generator 
    g_loss, _, _ = gan_model.train_on_batch(x_real_a, [y_real, x_real_b])
    print ('>%d, d1[%.3f] d2[%.3f] g[%.3f] '%(i+1, d_loss_1, d_loss_2, g_loss))

def preprocess_data (data): 
  x1,x2= data[0], data[1] 
  #scale 0 to 255 to [-1,1] 
  x1= (x1-127.5)/127.5 
  x2= (x2-127.5)/127.5 
  return [x1,x2]

data= [src_images, tar_images]

dataset= preprocess_data (data)

image_shape= src_images.shape[1:] 
print (image_shape)

d_model= discriminator (image_shape)

d_model.summary()

gan_model= define_gan (gen_model, d_model, image_shape)

train (d_model, gen_model, gan_model, dataset, n_epochs=10, n_batch=1)

def plot_images (src_image, gen_img, tar_img): 
  images= vstack ((src_image, gen_img, tar_img))
  # scale from [-1,1] to [0,1] 
  images= (images+1)/2.0 
  titles=['Source', 'Generated', 'Expected'] 
  for i in range (len(images)): 
    # define subplot 
    plt.subplot (1,3,i+1) 
    plt.axis ('off') 
    plt.imshow (images[i]) 
    plt.title (titles[i]) 
  plt.show ()

[x1,x2]= dataset

ix= randint (0, len(x1),1)
src_image, tar_image= x1[ix], x2[ix] 
gen_image= gen_model.predict (src_image)

plot_images (src_image, gen_image, tar_image)

